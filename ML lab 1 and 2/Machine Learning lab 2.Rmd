---
title: "Machine Learning - Learning Lab 2"
author: ""
output:
  html_document: default
  pdf_document: default
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

We will use the dataset (i.e, `dat_csv_combine_final.csv`) that we
created in ML lab 1 to run a logistic regression experiment.
{tidymodels} consists of several core packages, including {rsample} (for
sample splitting (e.g. train/test or cross-validation)), recipes (for
pre-processing), {parsnip} (for specifying the model), and yardstick
(for evaluating the model). let's first load the {tidymodels} package.

### Load the package

```{r}
library(tidymodels)
library(readr)
library(glmnet)
library(here)
```

### Load the dataset

```{r}
dat_csv_combine <- read_csv(here("data", "dat_csv_combine_final.csv"))

dat_csv_combine <- dat_csv_combine %>% 
  mutate(isSTEM = as.character(isSTEM))
```

Check the dimension of the data set using glimpse:

```{r}
glimpse(dat_csv_combine)
```

#### [Your Turn]{style="color: green;"} ⤵ {style="font-style: normal; font-variant-caps: normal; letter-spacing: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-tap-highlight-color: rgba(26, 26, 26, 0.3); -webkit-text-size-adjust: auto; -webkit-text-stroke-width: 0px; text-decoration: none; caret-color: rgb(0, 0, 0); color: rgb(0, 0, 0);"}

Like earlier, create a code chunk or use the console to answer the
following:

-   How many students and variables does this dataset have?
-   What are the variables used in the experiment of the paper
    (discussed in ML lab 1)?

Next, let's check details about this dataset using the handy `count()`
function, which works as follows

```{r}
dat_csv_combine %>% 
  count(isSTEM)
```

We can see, in this dataset, 164 students chose STEM and 350 students
chose non-STEM.

#### [Your Turn]{style="color: green;"} ⤵ {style="font-style: normal; font-variant-caps: normal; letter-spacing: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-tap-highlight-color: rgba(26, 26, 26, 0.3); -webkit-text-size-adjust: auto; -webkit-text-stroke-width: 0px; text-decoration: none; caret-color: rgb(0, 0, 0); color: rgb(0, 0, 0);"}

-   Without a model, just with random guess, how likely you would make a
    correct prediction of whether a student would choose STEM?

This is an imbalanced dataset (164:350). While there are several methods
for combating this issue using recipes (search for steps to upsample or
downsample) or other more specialized packages like themis, the analyses
shown below analyze the data as-is.

Now, look closely at the values of features and you will notice that the
range of "number of actions" is not the same as the other variables. We
should scale it so that they have the same range. We won't use tidyverse
code (and the `mutate()` function) below, as it's a bit easier to write
this code this way, using the `$` operator to directly change the
variable.

```{r}
dat_csv_combine$NumActions <- rescale(dat_csv_combine$NumActions, 
                                      to = c(0, 1), 
                                      from = range(dat_csv_combine$NumActions, 
                                                   na.rm = TRUE, 
                                                   finite = TRUE))
```

### Data splitting & resampling

For a data splitting strategy, let's reserve 25% of the data to the test
set. Since we have an imbalanced data set, we'll use a stratified random
sample:

```{r}
set.seed(123) #set.seed is used so that we can reproduce the result since splitting might be different every time we use initial_split function

splits <- initial_split(dat_csv_combine, strata = isSTEM)

data_other <- training(splits) #this dataset will be used to build models

data_test  <- testing(splits) #we will never use this dataset in the process of building models. This dataset is only for final test!
```

The initial_split() function is specially built to separate the data set
into a training and testing set. By default, it holds 3/4 of the data
for training and the rest for testing. That can be changed by passing
the prop argument. For instance,
`splits <- initial_split(dat_csv_combine, prop = 0.6)`.

Here is training set proportions by `isSTEM:`

```{r}
data_other %>% 
  count(isSTEM)
```

Here is test set proportions by `isSTEM`:

```{r}
data_test  %>% 
  count(isSTEM)
```

## Single resample

First, let's create a single resample called a validation set. In
{tidymodels}, a validation set is treated as a single iteration of
resampling. This will be a split from the 386 students that were not
used for testing, which we called data_other. This split creates two new
data sets:

-   the set held out for the purpose of measuring performance, called
    the validation set, and

-   the remaining data used to fit the model, called the training set.

We'll use the `validation_split()` function to allocate 20% of the
data_other dataset to the validation set and the remaining 80% to the
training set. This means that our model performance metrics will be
computed on a single set of 76 (or 78) instances (20%\*386). This is
fairly small, we usually would not do single resample with such as small
dataset. Here, we are doing it so that we know how to run a single
resample experiment.

```{r}
set.seed(234)
# Create data split for train and test
data_other_split <- initial_split(data_other,
                                prop = 0.8,
                                strata = isSTEM)

# Create training data
data_other_train <- data_other_split %>%
                    training()

# Create testing data
data_other_validation <- data_other_split %>%
                    testing()

# Checking the number of rows in train and test dataset
nrow(data_other_train)
nrow(data_other_validation)
```

This function, like initial_split(), has the same strata argument, which
uses stratified sampling to create the resample. This means that we'll
have roughly the same proportions of students who chose and did not
choose STEM in our new validation and training sets, as compared to the
original data_other proportions.

## A first model

Since our outcome variable `isSTEM` is categorical, logistic regression
would be a good first model to start. This is also the algorithm in the
paper. For logistic regression, the predicted label should be factor.

```{r}
data_other_train <- data_other_train %>% 
  mutate(isSTEM = as.factor(isSTEM))

data_other_validation <- data_other_validation %>% 
  mutate(isSTEM = as.factor(isSTEM))
```

Next, let's create the model with data_other_train.

```{r}
fitted_logistic_model<- logistic_reg() %>%
        # Set the engine
        set_engine("glm") %>%
        # Set the mode
        set_mode("classification") %>%
        # Fit the model
        fit(isSTEM~., data = data_other_train) #the training data is data_other_train and the predicted label is isSTEM
```

Now, let's take a look at the model

```{r}
temp <- tidy(fitted_logistic_model)    # Generate Summary Table
```

#### [Your Turn]{style="color: green;"} ⤵ {style="font-style: normal; font-variant-caps: normal; letter-spacing: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-tap-highlight-color: rgba(26, 26, 26, 0.3); -webkit-text-size-adjust: auto; -webkit-text-stroke-width: 0px; text-decoration: none; caret-color: rgb(0, 0, 0); color: rgb(0, 0, 0);"}

-   What do you see in the table?

You might notice that not all features are significant predictors as we
have a really small dataset for training the data. It means that with
this model, our prediction might not be accurate. Let's take a look at
the accuracy. We will use data_other_validation to get the model
performance.

```{r}
# Class prediction
pred_class <- predict(fitted_logistic_model,
                      new_data = data_other_validation,
                      type = "class")

pred_class[1:5,] # this gives us the first 5 predicted results
```

Let's compare the predicted result and the true value:

```{r}
prediction_results <- data_other_validation %>%
  select(isSTEM) %>%
  bind_cols(pred_class)

prediction_results[1:5, ] # this gives us the first 5 true values versus predicted results
```

#### [Your Turn]{style="color: green;"} ⤵ {style="font-style: normal; font-variant-caps: normal; letter-spacing: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-tap-highlight-color: rgba(26, 26, 26, 0.3); -webkit-text-size-adjust: auto; -webkit-text-stroke-width: 0px; text-decoration: none; caret-color: rgb(0, 0, 0); color: rgb(0, 0, 0);"}

-   What do you notice?

Next, let's take a look at the accuracy of the model with function
accuracy. We can use percent accuracy and chance corrected accuracy
(i.e., Kappa) to evaluate the model. We can get the percent accuracy:

```{r}
accuracy(prediction_results, truth = isSTEM,
         estimate = .pred_class)
```

We can get the Kappa:

```{r}
kap(prediction_results, truth = isSTEM,
         estimate = .pred_class)
```

#### [Your Turn]{style="color: green;"} ⤵ {style="font-style: normal; font-variant-caps: normal; letter-spacing: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-tap-highlight-color: rgba(26, 26, 26, 0.3); -webkit-text-size-adjust: auto; -webkit-text-stroke-width: 0px; text-decoration: none; caret-color: rgb(0, 0, 0); color: rgb(0, 0, 0);"}

-   Why Kappa is much smaller than percent accuracy?

We notice that the model is not doing well. Next, we want to closely
look at the confusion matrix to analyze the model performance.

```{r}
conf_mat(prediction_results, truth = isSTEM,
         estimate = .pred_class)
```

#### [Your Turn]{style="color: green;"} ⤵ {style="font-style: normal; font-variant-caps: normal; letter-spacing: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-tap-highlight-color: rgba(26, 26, 26, 0.3); -webkit-text-size-adjust: auto; -webkit-text-stroke-width: 0px; text-decoration: none; caret-color: rgb(0, 0, 0); color: rgb(0, 0, 0);"}

-   What do you notice?

## Reach

You have now built a machine learning model!

For this learning lab, your reach is to build a logistic regression
model with your own data set.
